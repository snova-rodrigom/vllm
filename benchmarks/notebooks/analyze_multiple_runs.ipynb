{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac860685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f7a3f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>backend</th>\n",
       "      <th>model_id</th>\n",
       "      <th>tokenizer_id</th>\n",
       "      <th>num_prompts</th>\n",
       "      <th>request_rate</th>\n",
       "      <th>burstiness</th>\n",
       "      <th>max_concurrency</th>\n",
       "      <th>duration</th>\n",
       "      <th>completed</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_input_lens</th>\n",
       "      <th>median_input_lens</th>\n",
       "      <th>std_input_lens</th>\n",
       "      <th>p99_input_lens</th>\n",
       "      <th>mean_output_lens</th>\n",
       "      <th>median_output_lens</th>\n",
       "      <th>std_output_lens</th>\n",
       "      <th>p99_output_lens</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20250624-162606</td>\n",
       "      <td>vllm</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>16</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16.292616</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>19999.0</td>\n",
       "      <td>19999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19999.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20250624-162320</td>\n",
       "      <td>vllm</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>8</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>10.096462</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>19999.0</td>\n",
       "      <td>19999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19999.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20250624-162432</td>\n",
       "      <td>vllm</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>16</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2.606409</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20250624-162657</td>\n",
       "      <td>vllm</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>32</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>14.114119</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>128</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20250624-163136</td>\n",
       "      <td>vllm</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>64</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>9.786660</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>999.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date backend                          model_id  \\\n",
       "0  20250624-162606    vllm  meta-llama/Llama-3.2-1B-Instruct   \n",
       "1  20250624-162320    vllm  meta-llama/Llama-3.2-1B-Instruct   \n",
       "2  20250624-162432    vllm  meta-llama/Llama-3.2-1B-Instruct   \n",
       "3  20250624-162657    vllm  meta-llama/Llama-3.2-1B-Instruct   \n",
       "4  20250624-163136    vllm  meta-llama/Llama-3.2-1B-Instruct   \n",
       "\n",
       "                       tokenizer_id  num_prompts request_rate  burstiness  \\\n",
       "0  meta-llama/Llama-3.2-1B-Instruct           16          inf         1.0   \n",
       "1  meta-llama/Llama-3.2-1B-Instruct            8          inf         1.0   \n",
       "2  meta-llama/Llama-3.2-1B-Instruct           16          inf         1.0   \n",
       "3  meta-llama/Llama-3.2-1B-Instruct           32          inf         1.0   \n",
       "4  meta-llama/Llama-3.2-1B-Instruct           64          inf         1.0   \n",
       "\n",
       "   max_concurrency   duration  completed  ...  mean_input_lens  \\\n",
       "0               16  16.292616         16  ...          19999.0   \n",
       "1                8  10.096462          8  ...          19999.0   \n",
       "2               16   2.606409         16  ...            999.0   \n",
       "3               32  14.114119         32  ...            127.0   \n",
       "4               64   9.786660         64  ...            999.0   \n",
       "\n",
       "   median_input_lens  std_input_lens p99_input_lens  mean_output_lens  \\\n",
       "0            19999.0             0.0        19999.0            2000.0   \n",
       "1            19999.0             0.0        19999.0            2000.0   \n",
       "2              999.0             0.0          999.0            1000.0   \n",
       "3              127.0             0.0          127.0            4096.0   \n",
       "4              999.0             0.0          999.0            2000.0   \n",
       "\n",
       "   median_output_lens  std_output_lens  p99_output_lens  input_tokens  \\\n",
       "0              2000.0              0.0           2000.0         20000   \n",
       "1              2000.0              0.0           2000.0         20000   \n",
       "2              1000.0              0.0           1000.0          1000   \n",
       "3              4096.0              0.0           4096.0           128   \n",
       "4              2000.0              0.0           2000.0          1000   \n",
       "\n",
       "   output_tokens  \n",
       "0           2000  \n",
       "1           2000  \n",
       "2           1000  \n",
       "3           4096  \n",
       "4           2000  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load JSON\n",
    "\n",
    "output_xlsx_file_name = 'combined_results-llama-3.2-1b_tp1_bf16_random.xlsx'\n",
    "json_files_dir = '../results/llama3.2-1b/llama3.2-1b-tp1-bf16-nocaching-random'\n",
    "\n",
    "json_dfs = []\n",
    "\n",
    "for json_file in os.listdir(json_files_dir):\n",
    "    if not json_file.endswith('.json'):\n",
    "        continue\n",
    "    \n",
    "    with open(os.path.join(json_files_dir, json_file), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Remove unwanted fields\n",
    "    unwanted_fields = {\"ttfts\", \"itls\", \"generated_texts\", \"errors\"}\n",
    "    data = {k: v for k, v in data.items() if k not in unwanted_fields}\n",
    "\n",
    "    # Compute statistics for input_lens and output_lens\n",
    "    for key in [\"input_lens\", \"output_lens\"]:\n",
    "        if key in data:\n",
    "            values = np.array(data[key])\n",
    "            prefix = \"input\" if key == \"input_lens\" else \"output\"\n",
    "            data[f\"mean_{prefix}_lens\"] = np.mean(values)\n",
    "            data[f\"median_{prefix}_lens\"] = np.median(values)\n",
    "            data[f\"std_{prefix}_lens\"] = np.std(values)\n",
    "            data[f\"p99_{prefix}_lens\"] = np.percentile(values, 99)\n",
    "            del data[key]  # Remove the original list\n",
    "    \n",
    "    # Add requested input and output tokens, and concurrency\n",
    "    input_tokens = json_file.split('_')[-4]\n",
    "    output_tokens = json_file.split('_')[-3]\n",
    "    \n",
    "    data[\"input_tokens\"] = int(input_tokens)\n",
    "    data[\"output_tokens\"] = int(output_tokens)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_data = pd.DataFrame([data])\n",
    "    \n",
    "    json_dfs.append(df_data)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df = pd.concat(json_dfs, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(os.path.join(json_files_dir,output_xlsx_file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two line charts\n",
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# # Group by (input_tokens, output_tokens)\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# df_tp8.sort_values(by='concurrency', inplace=True)\n",
    "# df_tp4.sort_values(by='concurrency', inplace=True)\n",
    "\n",
    "# plt.plot(df_tp8['concurrency'], df_tp8['output_throughput'], marker='o', label='Llama 3.3 70B TP8 ShareGPT')\n",
    "# plt.plot(df_tp4['concurrency'], df_tp4['output_throughput'], marker='o', label='Llama 3.3 70B TP4 ShareGPT')\n",
    "# plt.title('Output Throughput vs Concurrency (Scatter)')\n",
    "\n",
    "# # Set x-axis ticks and log scale\n",
    "# concurrency_ticks = sorted(df['concurrency'].unique())\n",
    "# plt.xscale('log', base=2)\n",
    "# plt.xticks(concurrency_ticks, labels=[str(c) for c in concurrency_ticks])\n",
    "# plt.xlabel('Concurrency (log scale)')\n",
    "# plt.ylabel('Output Throughput')\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# # Add legend\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb21d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_aisol",
   "language": "python",
   "name": "vllm_aisol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
